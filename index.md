# AI 编译器教程：从理论到 200T 规模实践

## 关于本教程

本教程面向资深程序员和 AI 科学家，系统介绍 AI 编译器的设计与实现，特别关注自动驾驶和具身智能场景下的 200T 参数级模型编译优化。教程采用理论与实践结合的方式，包含大量数学推导和工程实践案例。

## 学习路径建议

### 快速路径（2-3 周）
- 第 1-3 章：基础概念
- 第 7-8 章：核心优化
- 第 17 章：动态 shape
- 第 23 章：案例研究

### 标准路径（6-8 周）
- 顺序学习第 1-17 章
- 选择性学习第 18-22 章中感兴趣的专题

### 深度路径（3-4 个月）
- 完整学习所有章节
- 完成所有练习题
- 参与开源项目实践

## 目录

### 第一部分：基础架构

#### 第 1 章：AI 编译器概述
- AI 编译器的定义与边界
- 与传统编译器的区别
- 编译器栈的层次结构
- 主流框架生态系统

#### 第 2 章：中间表示（IR）设计
- 多层 IR 的必要性
- 图 IR vs 指令 IR
- SSA 形式在 AI 编译器中的应用
- MLIR 方言设计原则

#### 第 3 章：计算图表示与分析
- 数据流图构建
- 控制流与数据依赖
- 活性分析与生命周期
- 别名分析基础

### 第二部分：内存与数据管理

#### 第 4 章：统一缓冲区设计
- 统一内存模型架构
- 零拷贝优化策略
- 内存池管理
- 碎片化问题与解决方案

#### 第 5 章：内存规划与分配
- 静态内存规划算法
- 张量生命周期分析
- 内存复用策略
- 峰值内存优化

#### 第 6 章：数据布局优化
- NCHW vs NHWC 选择策略
- 布局转换最小化
- Padding 与对齐优化
- 混合精度布局

### 第三部分：核心优化技术

#### 第 7 章：算子融合
- 垂直融合与水平融合
- 融合规则与模式匹配
- 内存带宽优化
- 融合的收益分析模型

#### 第 8 章：自动微分与梯度优化
- 前向模式 vs 反向模式
- 检查点策略
- 梯度累积优化
- 高阶导数支持

#### 第 9 章：并行化策略
- 数据并行编译优化
- 模型并行切分策略
- 流水线并行调度
- 混合并行模式

### 第四部分：硬件适配层

#### 第 10 章：协处理器设计原则
- 主处理器与协处理器通信
- 命令队列设计
- DMA 优化
- 异步执行模型

#### 第 11 章：多维 Stride DMA 利用
- 多维 DMA 描述符设计
- Stride 访问模式分析
- 张量切片与 DMA 映射
- 非连续内存传输优化
- 2D/3D DMA 引擎编程模型
- DMA 链表与批处理

#### 第 13 章：GPU 编译优化
- CUDA 核函数生成
- Warp 调度优化
- 共享内存使用
- Tensor Core 利用

#### 第 14 章：移动端与边缘设备优化
- Qualcomm Hexagon DSP 编译
- Apple Neural Engine 适配
- ARM NEON 优化
- 功耗感知编译

### 第五部分：高级专题

#### 第 15 章：NUMA 架构优化（一）
- NUMA 基础概念
- 亲和性设置
- 本地内存分配策略
- NUMA 感知的数据放置

#### 第 16 章：NUMA 架构优化（二）
- 跨 Socket 通信优化
- NUMA 平衡算法
- 大规模 Transformer 的 NUMA 优化
- 性能分析与调优

#### 第 17 章：动态 Shape 编译（一）
- 符号形状推导
- Shape 函数与约束
- 动态内存管理
- 桶化策略

#### 第 18 章：动态 Shape 编译（二）
- 运行时特化
- 重编译触发机制
- Shape 缓存策略
- 性能预测模型

#### 第 19 章：稀疏与变长数据支持
- 稀疏张量表示（COO, CSR, CSC）
- 稀疏算子实现
- 变长序列批处理
- 动态稀疏模式

#### 第 20 章：JIT 编译技术
- JIT vs AOT 权衡
- 热点检测与优化
- 编译缓存管理
- 分层编译策略

#### 第 21 章：高维张量别名分析
- Stride 张量的别名问题
- 区间分析方法
- 依赖性测试
- 优化机会识别

#### 第 22 章：投机执行支持
- 投机解码的编译支持
- 分支预测优化
- 回滚机制设计
- 性能与正确性权衡

### 第六部分：实战案例

#### 第 23 章：自动驾驶场景优化
- 实时性约束处理
- 多模态融合优化
- 安全关键路径识别
- 确定性执行保证

#### 第 24 章：具身智能编译挑战
- 感知-决策-控制循环优化
- 异构计算资源调度
- 低延迟推理优化
- 能效优化策略

#### 第 25 章：200T 模型编译实践
- 模型分片策略
- 通信优化
- 内存层级管理
- 检查点与恢复

## 主流框架对比

| 框架 | IR 设计 | 硬件支持 | 动态 Shape | JIT 支持 | 主要应用场景 |
|------|---------|----------|------------|----------|--------------|
| XLA | HLO | NVIDIA, TPU, CPU | 有限 | 是 | TensorFlow/JAX |
| TVM | Relay/TIR | 全平台 | 是 | 是 | 端到端优化 |
| Triton | Triton IR | NVIDIA | 有限 | 是 | 核函数开发 |
| MLIR | 多方言 | 可扩展 | 是 | 是 | 编译器基础设施 |
| TorchScript | Graph IR | 多平台 | 是 | 是 | PyTorch 部署 |
| CoreML | MIL | Apple | 是 | 否 | iOS/macOS |
| ONNX Runtime | ONNX | 全平台 | 是 | 部分 | 跨框架部署 |

## 硬件平台覆盖

| 硬件类型 | 代表产品 | 关键特性 | 编译重点 |
|----------|----------|----------|----------|
| 数据中心 GPU | NVIDIA H100/H200 | HBM3, Tensor Core | 大批量并行, 混合精度 |
| | AMD MI300X | HBM3, Matrix Core | ROCm 优化 |
| AI 专用芯片 | Google TPU v5 | 超大片上内存 | XLA 优化 |
| | Tesla Dojo | 自定义互联 | 分布式编译 |
| 移动端 | Qualcomm Snapdragon | Hexagon DSP | 功耗优化 |
| | Apple M系列 | Neural Engine | CoreML 适配 |
| | MediaTek Dimensity | APU | 异构调度 |
| 边缘设备 | NVIDIA Jetson | CUDA + DLA | 实时性优化 |
| | Intel Movidius | VPU | 视觉任务优化 |

## 数学符号约定

- $\mathcal{G} = (V, E)$：计算图，其中 $V$ 为节点集，$E$ 为边集
- $\mathcal{T}$：张量空间
- $\phi: \mathcal{T} \rightarrow \mathcal{T}$：算子变换
- $M(\cdot)$：内存占用函数
- $L(\cdot)$：延迟函数
- $\mathcal{O}(\cdot)$：计算复杂度
- $\nabla$：梯度算子
- $\odot$：逐元素乘法
- $\otimes$：张量积

## 代码约定

本教程不包含具体代码实现，所有算法以伪代码或数学形式呈现。实际实现请参考各框架官方文档。

## 前置知识

建议读者具备以下基础：
- 编译原理基础（词法分析、语法分析、优化）
- 深度学习基础（反向传播、优化器、常见模型）
- 并行计算基础（CUDA/OpenCL 编程经验）
- 计算机体系结构（缓存、流水线、SIMD）

## 如何使用本教程

1. **循序渐进**：建议按章节顺序学习，每章都建立在前序知识基础上
2. **动手实践**：完成每章练习题，巩固理解
3. **查阅对照**：对照实际框架源码加深理解
4. **参与讨论**：加入社区讨论，分享学习心得

## 版权与致谢

本教程采用 CC BY-SA 4.0 协议发布。感谢 AI 编译器社区的持续贡献。

---

*开始学习：[第 1 章：AI 编译器概述](chapter1.md)*