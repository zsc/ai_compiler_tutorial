<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第 15 章：NUMA 架构优化（一）</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">AI 编译器教程：从理论到 200T 规模实践</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 1 章：AI 编译器概述</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 2 章：中间表示（IR）设计</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 3 章：计算图表示与分析</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 4 章：统一缓冲区设计</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 5 章：内存规划与分配</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 6 章：数据布局优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 7 章：算子融合</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 8 章：自动微分与梯度优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 9 章：并行化策略</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 11 章：多维 Stride DMA 利用</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 12 章：JIT 编译技术</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter13.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 13 章：GPU 编译优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter14.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 14 章：移动端与边缘设备优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter15.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 15 章：NUMA 架构优化（一）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter16.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 16 章：NUMA 架构优化（二）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter17.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 17 章：动态 Shape 编译（一）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter18.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 18 章：动态 Shape 编译（二）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter19.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 19 章：稀疏与变长数据支持</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter20.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 20 章：JIT 编译技术</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter21.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 21 章：高维张量别名分析</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter22.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 22 章：投机执行支持</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter23.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 23 章：自动驾驶场景优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter24.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 24 章：具身智能编译挑战</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter25.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 25 章：200T 模型编译实践</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="15-numa">第 15 章：NUMA 架构优化（一）</h1>
<h2 id="_1">章节概述</h2>
<p>本章深入探讨 NUMA（Non-Uniform Memory Access，非统一内存访问）架构下的 AI 编译器优化技术。随着 200T 参数级模型的出现，单一计算节点已无法满足计算和内存需求，NUMA 架构成为高性能计算的必然选择。本章将从 NUMA 基础概念出发，详细讨论亲和性设置、本地内存分配策略以及 NUMA 感知的数据放置技术，为读者在多 Socket 系统上优化 AI 工作负载提供理论基础和实践指导。</p>
<h2 id="151-numa">15.1 NUMA 基础概念</h2>
<h3 id="1511-numa">15.1.1 NUMA 架构演进</h3>
<p>传统的 SMP（Symmetric Multi-Processing）系统中，所有处理器通过共享总线访问统一的内存空间，这种架构在处理器数量增加时会遇到严重的总线竞争问题。NUMA 架构通过将内存分布到各个处理器节点，每个节点拥有本地内存，从而解决了这一瓶颈。</p>
<div class="codehilite"><pre><span></span><code>    传统 SMP 架构                    NUMA 架构

    CPU0  CPU1  CPU2  CPU3          Node 0          Node 1
      |     |     |     |           ┌─────────┐    ┌─────────┐
      └──┬──┴──┬──┴──┬──┘           │ CPU0-1  │    │ CPU2-3  │
         │     │     │              │ Memory0 │    │ Memory1 │
    ─────┴─────┴─────┴─────         └────┬────┘    └────┬────┘
         Shared Memory                    └──────┬───────┘
                                              QPI/UPI
</code></pre></div>

<h3 id="1512">15.1.2 内存访问延迟层次</h3>
<p>在 NUMA 系统中，内存访问延迟呈现明显的层次结构：</p>
<p>$$L_{access} = \begin{cases}
L_{local} &amp; \text{if } node(CPU) = node(Memory) \\
L_{remote} &amp; \text{if } node(CPU) \neq node(Memory)
\end{cases}$$
其中，$L_{remote} \approx \alpha \cdot L_{local}$，$\alpha$ 通常在 1.5 到 2.5 之间，具体取决于互连技术（如 Intel QPI/UPI、AMD Infinity Fabric）。</p>
<h3 id="1513-numa">15.1.3 NUMA 距离矩阵</h3>
<p>系统通过 NUMA 距离矩阵描述节点间的访问代价：
$$D = \begin{bmatrix}
d_{00} &amp; d_{01} &amp; \cdots &amp; d_{0n} \\
d_{10} &amp; d_{11} &amp; \cdots &amp; d_{1n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
d_{n0} &amp; d_{n1} &amp; \cdots &amp; d_{nn}
\end{bmatrix}$$
其中 $d_{ij}$ 表示从节点 $i$ 访问节点 $j$ 内存的相对延迟。对角线元素 $d_{ii} = 10$（标准化本地访问），非对角线元素通常为 20 或 21。</p>
<h3 id="1514-numa">15.1.4 NUMA 拓扑发现</h3>
<p>编译器需要在编译时或运行时发现 NUMA 拓扑结构。关键信息包括：</p>
<ul>
<li><strong>节点数量</strong>：$N_{nodes}$</li>
<li><strong>每节点 CPU 数</strong>：$C_{per_node}$</li>
<li><strong>每节点内存容量</strong>：$M_{per_node}$</li>
<li><strong>节点间带宽矩阵</strong>：$B_{ij}$</li>
</ul>
<p>这些信息通过系统调用（如 Linux 的 <code>libnuma</code>）或硬件抽象层获取。</p>
<h2 id="152">15.2 亲和性设置</h2>
<h3 id="1521-cpu">15.2.1 CPU 亲和性</h3>
<p>CPU 亲和性控制线程到特定 CPU 核心的绑定，是 NUMA 优化的基础：
$$\text{Affinity}: T \rightarrow \mathcal{P}(C)$$
其中 $T$ 是线程集合，$C$ 是 CPU 核心集合，$\mathcal{P}(C)$ 是 $C$ 的幂集。</p>
<p>亲和性策略包括：</p>
<ol>
<li>
<p><strong>紧密绑定（Compact）</strong>：将线程绑定到同一 NUMA 节点
   - 优势：最小化内存访问延迟
   - 劣势：可能造成节点内资源竞争</p>
</li>
<li>
<p><strong>分散绑定（Scatter）</strong>：将线程均匀分布到各节点
   - 优势：均衡使用内存带宽
   - 劣势：增加跨节点通信开销</p>
</li>
<li>
<p><strong>层次绑定（Hierarchical）</strong>：基于任务依赖关系的智能绑定</p>
</li>
</ol>
<h3 id="1522">15.2.2 内存亲和性</h3>
<p>内存亲和性策略决定数据的物理放置位置：
$$\text{Placement}: M \rightarrow N$$
其中 $M$ 是内存页集合，$N$ 是 NUMA 节点集合。</p>
<p>常见策略：</p>
<ol>
<li><strong>首次触碰（First-Touch）</strong>：页面分配到首次访问它的线程所在节点</li>
<li><strong>交错放置（Interleave）</strong>：页面轮流分配到各节点</li>
<li><strong>绑定放置（Bind）</strong>：显式指定页面所属节点</li>
</ol>
<h3 id="1523">15.2.3 亲和性优化模型</h3>
<p>给定计算图 $\mathcal{G} = (V, E)$ 和 NUMA 拓扑，亲和性优化问题可形式化为：
$$\min_{f: V \rightarrow N} \sum_{(u,v) \in E} w_{uv} \cdot d_{f(u),f(v)} + \sum_{v \in V} c_v \cdot l_{f(v)}$$
其中：</p>
<ul>
<li>$w_{uv}$ 是边 $(u,v)$ 的通信量</li>
<li>$d_{ij}$ 是节点 $i$ 到 $j$ 的 NUMA 距离</li>
<li>$c_v$ 是顶点 $v$ 的计算量</li>
<li>$l_i$ 是节点 $i$ 的负载</li>
</ul>
<p>这是一个 NP-hard 问题，实践中使用启发式算法求解。</p>
<h2 id="153">15.3 本地内存分配策略</h2>
<h3 id="1531">15.3.1 静态分配策略</h3>
<p>编译时确定的内存分配策略，基于程序分析和性能模型：</p>
<ol>
<li>
<p><strong>数据分区</strong>：将大型张量按 NUMA 节点数分区
$$T_{large} = T_0 \oplus T_1 \oplus \cdots \oplus T_{n-1}$$
其中 $T_i$ 分配到节点 $i$</p>
</li>
<li>
<p><strong>复制策略</strong>：对频繁访问的只读数据进行复制
$$\text{Replicate}(T_{readonly}) = \{T^{(0)}, T^{(1)}, \ldots, T^{(n-1)}\}$$</p>
</li>
<li>
<p><strong>迁移策略</strong>：基于访问模式的数据迁移
$$\text{Migrate}(T, n_{src}, n_{dst}) \text{ if } A_{dst}(T) &gt; \theta \cdot A_{src}(T)$$
其中 $A_i(T)$ 是节点 $i$ 对张量 $T$ 的访问频率</p>
</li>
</ol>
<h3 id="1532">15.3.2 动态分配策略</h3>
<p>运行时根据系统状态调整的分配策略：</p>
<ol>
<li>
<p><strong>自适应页面迁移</strong>：
$$P_{migrate} = \frac{R_{remote}}{R_{remote} + R_{local}} &gt; \tau$$
当远程访问比例超过阈值 $\tau$ 时触发迁移</p>
</li>
<li>
<p><strong>内存压力均衡</strong>：
$$\text{Balance}(M_i) = M_{avg} \pm \delta$$
保持各节点内存使用量在平均值 $M_{avg}$ 的 $\delta$ 范围内</p>
</li>
<li>
<p><strong>带宽感知分配</strong>：
$$node_{alloc} = \arg\min_i \frac{BW_{used}^{(i)}}{BW_{max}^{(i)}}$$
选择带宽利用率最低的节点进行分配</p>
</li>
</ol>
<h3 id="1533">15.3.3 大页面优化</h3>
<p>使用大页面（Huge Pages）减少 TLB 失效，在 NUMA 系统中尤为重要：
$$\text{TLB_Coverage} = \text{Page_Size} \times \text{TLB_Entries}$$
标准页面（4KB）vs 大页面（2MB/1GB）：</p>
<ul>
<li>减少页表遍历开销</li>
<li>降低 TLB 失效率</li>
<li>简化 NUMA 页面管理</li>
</ul>
<p>大页面分配策略需要考虑：</p>
<ol>
<li>内存碎片化风险</li>
<li>NUMA 节点内存容量限制</li>
<li>页面共享与迁移的粒度权衡</li>
</ol>
<h2 id="154-numa">15.4 NUMA 感知的数据放置</h2>
<h3 id="1541">15.4.1 张量分布模型</h3>
<p>对于大规模张量，需要设计 NUMA 感知的分布策略：</p>
<ol>
<li>
<p><strong>块分布（Block Distribution）</strong>：
$$T[i:j] \rightarrow node_k \text{ where } k = \lfloor \frac{i \cdot N_{nodes}}{size(T)} \rfloor$$</p>
</li>
<li>
<p><strong>循环分布（Cyclic Distribution）</strong>：
$$T[i] \rightarrow node_{(i \mod N_{nodes})}$$</p>
</li>
<li>
<p><strong>块循环分布（Block-Cyclic）</strong>：
   结合块分布和循环分布的优点
$$T[b \cdot B + offset] \rightarrow node_{(b \mod N_{nodes})}$$
其中 $B$ 是块大小</p>
</li>
</ol>
<h3 id="1542-">15.4.2 计算-数据协同放置</h3>
<p>将计算任务与其访问的数据放置在同一 NUMA 节点：</p>
<p><strong>局部性度量</strong>：
$$\text{Locality}(t, n) = \frac{\sum_{d \in D_t \cap M_n} size(d)}{\sum_{d \in D_t} size(d)}$$
其中 $D_t$ 是任务 $t$ 访问的数据集，$M_n$ 是节点 $n$ 的本地内存。</p>
<p><strong>优化目标</strong>：
$$\max \sum_{t \in T} \sum_{n \in N} x_{tn} \cdot \text{Locality}(t, n)$$
约束条件：</p>
<ul>
<li>$\sum_n x_{tn} = 1$ （每个任务分配到一个节点）</li>
<li>$\sum_t x_{tn} \cdot comp(t) \leq capacity(n)$ （节点容量限制）</li>
</ul>
<h3 id="1543">15.4.3 多级缓存优化</h3>
<p>NUMA 系统的多级缓存层次需要特殊考虑：</p>
<div class="codehilite"><pre><span></span><code>    L1 Cache (32KB)
         ↓
    L2 Cache (256KB) 
         ↓
    L3 Cache (30MB, shared within socket)
         ↓
    Local Memory (DDR4/5)
         ↓
    Remote Memory (via QPI/UPI)
</code></pre></div>

<p>缓存优化策略：</p>
<ol>
<li><strong>缓存着色（Cache Coloring）</strong>：避免关键数据的缓存冲突</li>
<li><strong>预取优化（Prefetching）</strong>：考虑 NUMA 延迟的预取时机</li>
<li><strong>伪共享消除</strong>：避免跨 NUMA 节点的缓存行共享</li>
</ol>
<h3 id="1544-transformer-numa">15.4.4 Transformer 模型的 NUMA 优化</h3>
<p>以 Transformer 模型为例，展示 NUMA 感知的数据放置：</p>
<ol>
<li>
<p><strong>注意力矩阵分块</strong>：
$$A = QK^T = \begin{bmatrix} Q_0 \\ Q_1 \\ \vdots \\ Q_{n-1} \end{bmatrix} \begin{bmatrix} K_0^T &amp; K_1^T &amp; \cdots &amp; K_{n-1}^T \end{bmatrix}$$
将 $Q_i$ 和 $K_i$ 放置在节点 $i$，减少跨节点通信</p>
</li>
<li>
<p><strong>FFN 层分区</strong>：
$$\text{FFN}(x) = \text{GELU}(xW_1)W_2$$
将 $W_1$ 按列分区，$W_2$ 按行分区，实现计算的 NUMA 局部性</p>
</li>
<li>
<p><strong>All-Reduce 优化</strong>：
   使用 NUMA 感知的树形规约算法，优先进行节点内规约</p>
</li>
</ol>
<h2 id="_2">本章小结</h2>
<p>本章系统介绍了 NUMA 架构下的 AI 编译器优化基础：</p>
<ol>
<li><strong>NUMA 基础概念</strong>：理解非统一内存访问的本质，掌握 NUMA 距离矩阵和拓扑发现方法</li>
<li><strong>亲和性设置</strong>：通过 CPU 和内存亲和性控制，优化线程和数据的物理放置</li>
<li><strong>本地内存分配</strong>：设计静态和动态分配策略，利用大页面优化减少开销</li>
<li><strong>数据放置优化</strong>：实现张量分布、计算-数据协同放置和多级缓存优化</li>
</ol>
<p>关键公式回顾：</p>
<ul>
<li>内存访问延迟：$L_{remote} \approx \alpha \cdot L_{local}$</li>
<li>亲和性优化：$\min \sum_{(u,v) \in E} w_{uv} \cdot d_{f(u),f(v)}$</li>
<li>局部性度量：$\text{Locality}(t, n) = \frac{\text{local_data}}{\text{total_data}}$</li>
</ul>
<p>下一章将继续深入探讨跨 Socket 通信优化、NUMA 平衡算法以及大规模 Transformer 的 NUMA 优化实践。</p>
<h2 id="_3">练习题</h2>
<h3 id="_4">基础题</h3>
<p><strong>练习 15.1</strong>：NUMA 距离矩阵计算</p>
<p>给定一个 4 节点 NUMA 系统，节点间通过环形拓扑连接，相邻节点间延迟为 20ns，本地访问延迟为 10ns。请构建该系统的 NUMA 距离矩阵。</p>
<p><em>提示：考虑最短路径，环形拓扑中对角节点需要经过两跳。</em></p>
<details>
<summary>答案</summary>
<p>距离矩阵为：
$$D = \begin{bmatrix}
10 &amp; 20 &amp; 30 &amp; 20 \\
20 &amp; 10 &amp; 20 &amp; 30 \\
30 &amp; 20 &amp; 10 &amp; 20 \\
20 &amp; 30 &amp; 20 &amp; 10
\end{bmatrix}$$
解释：</p>
<ul>
<li>对角线元素（本地访问）：10</li>
<li>相邻节点（如 0→1, 1→2, 2→3, 3→0）：20</li>
<li>对角节点（如 0→2, 1→3）：30（需要经过两跳）</li>
</ul>
</details>
<p><strong>练习 15.2</strong>：内存带宽计算</p>
<p>某 NUMA 系统有 2 个节点，每节点本地内存带宽为 100 GB/s，跨节点带宽为 40 GB/s。若一个应用 70% 的内存访问是本地的，30% 是远程的，计算实际可达到的平均内存带宽。</p>
<p><em>提示：使用加权平均计算有效带宽。</em></p>
<details>
<summary>答案</summary>
<p>平均带宽 = 0.7 × 100 GB/s + 0.3 × 40 GB/s = 70 + 12 = 82 GB/s</p>
<p>带宽效率 = 82 / 100 = 82%</p>
<p>这说明即使只有 30% 的远程访问，也会导致 18% 的带宽损失。</p>
</details>
<p><strong>练习 15.3</strong>：页面迁移决策</p>
<p>一个 4KB 页面在过去 1000 次访问中，本地访问 200 次（每次 10ns），远程访问 800 次（每次 20ns）。页面迁移开销为 10μs。计算是否应该迁移该页面。</p>
<p><em>提示：比较迁移前后的总开销。</em></p>
<details>
<summary>答案</summary>
<p>当前总延迟：200 × 10ns + 800 × 20ns = 2000ns + 16000ns = 18μs</p>
<p>迁移后（假设访问模式不变）：</p>
<ul>
<li>迁移开销：10μs</li>
<li>新的访问延迟：800 × 10ns + 200 × 20ns = 8000ns + 4000ns = 12μs</li>
<li>总开销：10μs + 12μs = 22μs</li>
</ul>
<p>结论：不应迁移，因为 22μs &gt; 18μs。</p>
<p>但如果考虑未来多次访问，设访问次数为 n：</p>
<ul>
<li>不迁移：18n μs</li>
<li>迁移：10 + 12n μs</li>
</ul>
<p>当 18n &gt; 10 + 12n，即 n &gt; 1.67 时，迁移有利。</p>
</details>
<h3 id="_5">挑战题</h3>
<p><strong>练习 15.4</strong>：最优数据分区</p>
<p>给定一个 1TB 的张量和 4 个 NUMA 节点（每节点 256GB 内存），设计最优的数据分区方案。已知访问模式为：前 25% 数据访问频率是后 75% 的 4 倍。</p>
<p><em>提示：考虑访问频率加权的负载均衡。</em></p>
<details>
<summary>答案</summary>
<p>设前 25% 数据（256GB）的访问权重为 4，后 75% 数据（768GB）的访问权重为 1。</p>
<p>总访问权重 = 256GB × 4 + 768GB × 1 = 1024 + 768 = 1792</p>
<p>每节点应承担权重 = 1792 / 4 = 448</p>
<p>分区方案：</p>
<ul>
<li>节点 0：前 112GB（权重 448）</li>
<li>节点 1：接下来 112GB（权重 448）  </li>
<li>节点 2：接下来 32GB（权重 128）+ 后部分 320GB（权重 320）</li>
<li>节点 3：最后 448GB（权重 448）</li>
</ul>
<p>这样每个节点的访问负载基本均衡。</p>
</details>
<p><strong>练习 15.5</strong>：Transformer 注意力机制的 NUMA 优化</p>
<p>对于一个序列长度 L=8192，隐藏维度 d=4096 的自注意力层，在 4 节点 NUMA 系统上如何分配 Q、K、V 矩阵以最小化通信开销？每个矩阵大小为 L×d×4 字节（float32）。</p>
<p><em>提示：考虑注意力计算 $A = QK^T$ 的通信模式。</em></p>
<details>
<summary>答案</summary>
<p>每个矩阵大小 = 8192 × 4096 × 4 = 128MB</p>
<p>策略 1：序列维度分区</p>
<ul>
<li>将序列分成 4 份，每份 2048 个 token</li>
<li>节点 i 存储 Q[2048i:2048(i+1), :], K[2048i:2048(i+1), :], V[2048i:2048(i+1), :]</li>
<li>计算 $QK^T$ 时需要 all-to-all 通信</li>
<li>通信量：每节点发送 K 的 3/4 = 96MB</li>
</ul>
<p>策略 2：头维度分区（假设多头注意力，h=32 头）</p>
<ul>
<li>每节点处理 8 个注意力头</li>
<li>节点 i 存储所有序列的第 8i 到 8(i+1)-1 头</li>
<li>无需通信即可完成注意力计算</li>
<li>通信量：0（最优）</li>
</ul>
<p>结论：头维度分区更优，实现了完全的 NUMA 局部性。</p>
</details>
<p><strong>练习 15.6</strong>：动态负载均衡</p>
<p>设计一个 NUMA 感知的工作窃取算法，当某节点的任务队列为空时，如何决定从哪个节点窃取任务？给出决策函数。</p>
<p><em>提示：考虑 NUMA 距离和队列长度的权衡。</em></p>
<details>
<summary>答案</summary>
<p>决策函数：
$$\text{StealFrom}(i) = \arg\max_{j \neq i} \frac{Q_j - \tau}{d_{ij}}$$
其中：</p>
<ul>
<li>$Q_j$ 是节点 j 的队列长度</li>
<li>$\tau$ 是窃取阈值（如 2）</li>
<li>$d_{ij}$ 是 NUMA 距离</li>
</ul>
<p>改进版本（考虑数据局部性）：
$$\text{StealFrom}(i) = \arg\max_{j \neq i} \frac{(Q_j - \tau) \cdot (1 + \lambda \cdot L_{ij})}{d_{ij}}$$
其中 $L_{ij}$ 是节点 j 的任务访问节点 i 数据的比例，$\lambda$ 是局部性权重。</p>
<p>算法步骤：</p>
<ol>
<li>计算所有非空节点的得分</li>
<li>选择得分最高的节点</li>
<li>窃取其队列尾部的任务（更可能有好的局部性）</li>
<li>更新任务的数据亲和性信息</li>
</ol>
</details>
<p><strong>练习 15.7</strong>：内存分配器设计</p>
<p>设计一个 NUMA 感知的内存池，支持不同大小的内存块分配。要求：(1) 最小化跨节点分配，(2) 支持内存块迁移，(3) 避免碎片化。</p>
<p><em>提示：使用分级内存池和迁移策略。</em></p>
<details>
<summary>答案</summary>
<p>设计方案：</p>
<ol>
<li>
<p><strong>分级结构</strong>：
   - 小块池（&lt;4KB）：每 CPU 核心私有
   - 中块池（4KB-1MB）：每 NUMA 节点共享
   - 大块池（&gt;1MB）：全局池，NUMA 感知分配</p>
</li>
<li>
<p><strong>分配算法</strong>：</p>
</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="k">Allocate</span><span class="p">(</span><span class="k">size</span><span class="p">,</span><span class="w"> </span><span class="n">hint_node</span><span class="p">)</span><span class="err">:</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="k">size</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">4</span><span class="nl">KB</span><span class="p">:</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">CPU_local_pool</span><span class="p">.</span><span class="n">alloc</span><span class="p">(</span><span class="k">size</span><span class="p">)</span>
<span class="w">  </span><span class="n">elif</span><span class="w"> </span><span class="k">size</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">1</span><span class="nl">MB</span><span class="p">:</span>
<span class="w">    </span><span class="n">pool</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">NUMA_pools</span><span class="o">[</span><span class="n">hint_node</span><span class="o">]</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">pool</span><span class="p">.</span><span class="n">has_space</span><span class="p">(</span><span class="k">size</span><span class="p">)</span><span class="err">:</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">pool</span><span class="p">.</span><span class="n">alloc</span><span class="p">(</span><span class="k">size</span><span class="p">)</span>
<span class="w">    </span><span class="k">else</span><span class="err">:</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">find_nearest_pool</span><span class="p">(</span><span class="n">hint_node</span><span class="p">,</span><span class="w"> </span><span class="k">size</span><span class="p">)</span>
<span class="w">  </span><span class="k">else</span><span class="err">:</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">global_pool</span><span class="p">.</span><span class="n">alloc_numa_aware</span><span class="p">(</span><span class="k">size</span><span class="p">,</span><span class="w"> </span><span class="n">hint_node</span><span class="p">)</span>
</code></pre></div>

<ol start="3">
<li>
<p><strong>迁移策略</strong>：
   - 监控跨节点访问计数器
   - 当 remote_access / total_access &gt; 0.7 时触发迁移
   - 迁移时机：内存压力低时的后台任务</p>
</li>
<li>
<p><strong>防碎片化</strong>：
   - 使用 buddy allocator 或 slab allocator
   - 定期整理：合并相邻空闲块
   - 大页面优先：优先分配 2MB 大页</p>
</li>
</ol>
</details>
<p><strong>练习 15.8</strong>：性能建模</p>
<p>建立一个 NUMA 系统的性能预测模型，输入：计算图、数据大小、NUMA 拓扑；输出：预期执行时间。考虑计算、内存访问和通信的重叠。</p>
<p><em>提示：使用排队网络模型或 Roofline 模型的 NUMA 扩展。</em></p>
<details>
<summary>答案</summary>
<p>性能模型：
$$T_{exec} = \max(T_{comp}, T_{mem}, T_{comm})$$
其中：</p>
<ol>
<li>
<p><strong>计算时间</strong>：
$$T_{comp} = \sum_{v \in V} \frac{FLOPs(v)}{throughput_{node(v)}}$$</p>
</li>
<li>
<p><strong>内存访问时间</strong>：
$$T_{mem} = \sum_{v \in V} \left( \frac{M_{local}(v)}{BW_{local}} + \frac{M_{remote}(v)}{BW_{remote}} \right)$$</p>
</li>
<li>
<p><strong>通信时间</strong>：
$$T_{comm} = \sum_{(u,v) \in E} \frac{data(u,v)}{BW_{interconnect}} \cdot overlap_factor$$
<strong>重叠因子</strong>：
$$overlap_factor = 1 - \min(\alpha_{comp-mem}, \alpha_{mem-comm})$$
其中 $\alpha$ 表示重叠程度，通过硬件特性和访问模式估算。</p>
</li>
</ol>
<p><strong>NUMA 扩展的 Roofline 模型</strong>：
$$Performance = \min\left(Peak_FLOPs, \frac{AI \cdot BW_{eff}}{1 + \beta \cdot r_{remote}}\right)$$</p>
<p>其中：</p>
<ul>
<li>$AI$ 是算术强度</li>
<li>$BW_{eff}$ 是有效带宽</li>
<li>$r_{remote}$ 是远程访问比例</li>
<li>$\beta$ 是 NUMA 惩罚系数</li>
</ul>
</details>
<h2 id="gotchas">常见陷阱与错误 (Gotchas)</h2>
<h3 id="1">1. 首次触碰陷阱</h3>
<p><strong>问题</strong>：使用 <code>calloc</code> 或 <code>memset</code> 初始化大数组导致所有内存分配到单一节点。</p>
<p><strong>解决</strong>：使用并行初始化，让每个线程初始化其将要访问的部分。</p>
<h3 id="2">2. 错误的亲和性设置</h3>
<p><strong>问题</strong>：过度限制 CPU 亲和性导致负载不均衡。</p>
<p><strong>症状</strong>：某些核心 100% 利用率，其他核心空闲。</p>
<p><strong>解决</strong>：使用分层亲和性，允许在节点内迁移。</p>
<h3 id="3">3. 页面抖动</h3>
<p><strong>问题</strong>：频繁的页面迁移导致性能下降。</p>
<p><strong>症状</strong>：系统调用开销异常高。</p>
<p><strong>解决</strong>：设置迁移阈值和冷却期。</p>
<h3 id="4">4. 内存带宽瓶颈误判</h3>
<p><strong>问题</strong>：将 NUMA 远程访问延迟误认为是带宽不足。</p>
<p><strong>诊断</strong>：使用硬件计数器区分延迟限制和带宽限制。</p>
<h3 id="5">5. 不均匀的内存分配</h3>
<p><strong>问题</strong>：某些节点内存耗尽而其他节点有大量空闲。</p>
<p><strong>监控</strong>：定期检查 <code>/proc/meminfo</code> 的每节点统计。</p>
<h3 id="6-numa">6. 忽略 NUMA 距离的非对称性</h3>
<p><strong>问题</strong>：假设 $d_{ij} = d_{ji}$，但某些系统中这不成立。</p>
<p><strong>验证</strong>：始终检查完整的距离矩阵。</p>
<h3 id="7">7. 缓存伪共享</h3>
<p><strong>问题</strong>：不同 NUMA 节点的线程访问同一缓存行的不同部分。</p>
<p><strong>解决</strong>：使用填充（padding）或重新组织数据结构。</p>
<h3 id="8">8. 大页面分配失败</h3>
<p><strong>问题</strong>：运行时无法分配大页面，回退到标准页面。</p>
<p><strong>预防</strong>：启动时预留大页面，使用 <code>hugetlbfs</code>。</p>
<h2 id="_6">最佳实践检查清单</h2>
<h3 id="_7">设计阶段</h3>
<ul>
<li>[ ] 分析应用的内存访问模式和通信模式</li>
<li>[ ] 评估数据并行 vs 模型并行的 NUMA 影响</li>
<li>[ ] 设计支持 NUMA 的数据结构（避免全局共享状态）</li>
<li>[ ] 规划内存容量需求，确保不超过节点容量</li>
</ul>
<h3 id="_8">实现阶段</h3>
<ul>
<li>[ ] 使用 NUMA 感知的内存分配器</li>
<li>[ ] 实现分层的线程池和任务调度器</li>
<li>[ ] 添加 NUMA 拓扑发现和自适应逻辑</li>
<li>[ ] 实现关键数据结构的 NUMA 分区版本</li>
</ul>
<h3 id="_9">优化阶段</h3>
<ul>
<li>[ ] 使用硬件计数器监控远程访问比例</li>
<li>[ ] 分析内存带宽利用率的均衡性</li>
<li>[ ] 检查 TLB 命中率和大页面使用情况</li>
<li>[ ] 验证 CPU 亲和性设置的有效性</li>
</ul>
<h3 id="_10">测试阶段</h3>
<ul>
<li>[ ] 在不同 NUMA 配置下测试（1/2/4/8 节点）</li>
<li>[ ] 测试内存压力下的行为</li>
<li>[ ] 验证故障转移和降级策略</li>
<li>[ ] 基准测试：对比 NUMA 优化前后的性能</li>
</ul>
<h3 id="_11">部署阶段</h3>
<ul>
<li>[ ] 文档化 NUMA 相关的系统要求</li>
<li>[ ] 提供 NUMA 配置的最佳实践指南</li>
<li>[ ] 实现 NUMA 性能指标的监控和告警</li>
<li>[ ] 准备 NUMA 相关问题的调试工具和流程</li>
</ul>
<h3 id="_12">维护阶段</h3>
<ul>
<li>[ ] 定期审查 NUMA 性能指标趋势</li>
<li>[ ] 跟踪硬件拓扑变化（如 CPU 热插拔）</li>
<li>[ ] 更新性能模型以反映实际运行数据</li>
<li>[ ] 收集和分析 NUMA 相关的性能问题案例</li>
</ul>
            </article>
            
            <nav class="page-nav"><a href="chapter14.html" class="nav-link prev">← 第 14 章：移动端与边缘设备优化</a><a href="chapter16.html" class="nav-link next">第 16 章：NUMA 架构优化（二） →</a></nav>
        </main>
    </div>
</body>
</html>