<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第 18 章：动态 Shape 编译（二）</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">AI 编译器教程：从理论到 200T 规模实践</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 1 章：AI 编译器概述</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 2 章：中间表示（IR）设计</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 3 章：计算图表示与分析</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 4 章：统一缓冲区设计</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 5 章：内存规划与分配</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 6 章：数据布局优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 7 章：算子融合</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 8 章：自动微分与梯度优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 9 章：并行化策略</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 11 章：多维 Stride DMA 利用</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 12 章：JIT 编译技术</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter13.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 13 章：GPU 编译优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter14.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 14 章：移动端与边缘设备优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter15.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 15 章：NUMA 架构优化（一）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter16.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 16 章：NUMA 架构优化（二）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter17.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 17 章：动态 Shape 编译（一）</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter18.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 18 章：动态 Shape 编译（二）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter19.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 19 章：稀疏与变长数据支持</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter20.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 20 章：JIT 编译技术</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter21.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 21 章：高维张量别名分析</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter22.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 22 章：投机执行支持</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter23.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 23 章：自动驾驶场景优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter24.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 24 章：具身智能编译挑战</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter25.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 25 章：200T 模型编译实践</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="18-shape">第 18 章：动态 Shape 编译（二）</h1>
<p>在上一章中，我们探讨了动态 shape 的静态分析技术，包括符号形状推导、约束求解和桶化策略。本章将深入运行时层面，研究如何通过运行时特化、智能重编译和缓存机制，在保持灵活性的同时实现接近静态 shape 的性能。这些技术对于自动驾驶中的可变目标检测和具身智能的动态环境感知至关重要。</p>
<h2 id="181">18.1 运行时特化</h2>
<p>运行时特化是动态 shape 编译的核心技术，通过在运行时根据实际形状生成优化代码，实现性能与灵活性的平衡。</p>
<h3 id="1811">18.1.1 特化时机决策</h3>
<p>特化时机的选择直接影响系统性能。过早特化会增加编译开销，过晚特化则错失优化机会。</p>
<p><strong>热度阈值模型</strong>：</p>
<p>设形状 $s$ 在时间窗口 $[t-w, t]$ 内出现次数为 $f(s,t,w)$，特化触发条件为：</p>
<p>$$\begin{cases}
\text{specialize}(s) &amp; \text{if } f(s,t,w) &gt; \theta_{\text{hot}} \\
\text{defer}(s) &amp; \text{if } \theta_{\text{cold}} &lt; f(s,t,w) \leq \theta_{\text{hot}} \\
\text{interpret}(s) &amp; \text{if } f(s,t,w) \leq \theta_{\text{cold}}
\end{cases}$$
其中 $\theta_{\text{hot}}$ 和 $\theta_{\text{cold}}$ 是动态调整的阈值。</p>
<p><strong>收益预测函数</strong>：
$$B(s) = (T_{\text{interp}}(s) - T_{\text{spec}}(s)) \times P_{\text{reuse}}(s) - C_{\text{compile}}(s)$$
其中：</p>
<ul>
<li>$T_{\text{interp}}(s)$：解释执行时间</li>
<li>$T_{\text{spec}}(s)$：特化代码执行时间  </li>
<li>$P_{\text{reuse}}(s)$：形状重用概率</li>
<li>$C_{\text{compile}}(s)$：编译开销</li>
</ul>
<h3 id="1812">18.1.2 代码生成策略</h3>
<p>运行时代码生成需要在编译质量和编译速度之间权衡。</p>
<p><strong>分层特化架构</strong>：</p>
<div class="codehilite"><pre><span></span><code>Level 0: 通用解释器
   ↓ (threshold: 10 calls)
Level 1: 基础特化（循环展开、边界检查消除）
   ↓ (threshold: 100 calls)
Level 2: 深度优化（向量化、预取优化）
   ↓ (threshold: 1000 calls)
Level 3: 极致优化（自动调优、硬件特定优化）
</code></pre></div>

<p><strong>特化代码模板</strong>：</p>
<p>对于卷积操作 $Y = X * W$，其中 $X \in \mathbb{R}^{N \times C \times H \times W}$：</p>
<p>通用模板：
$$Y_{n,k,i,j} = \sum_{c,u,v} X_{n,c,i+u,j+v} \cdot W_{k,c,u,v}$$
特化实例（当 $H=224, W=224$ 时）：</p>
<ul>
<li>消除边界检查</li>
<li>循环分块对齐缓存行</li>
<li>向量化内层循环</li>
</ul>
<h3 id="1813">18.1.3 内存管理优化</h3>
<p>动态 shape 的内存管理面临碎片化和分配开销问题。</p>
<p><strong>内存池设计</strong>：</p>
<p>采用分级内存池管理：
$$\text{Pool}_i = \{b | 2^i \leq \text{size}(b) &lt; 2^{i+1}\}$$
分配策略：
$$\text{allocate}(s) = \begin{cases}
\text{Pool}_{\lceil \log_2(s) \rceil} &amp; \text{if available} \\
\text{malloc}(s) &amp; \text{otherwise}
\end{cases}$$
<strong>预分配策略</strong>：</p>
<p>基于历史模式预测未来内存需求：
$$M_{\text{pred}}(t+1) = \alpha \cdot M_{\text{obs}}(t) + (1-\alpha) \cdot M_{\text{pred}}(t)$$
其中 $\alpha$ 是平滑因子，通常取 $0.3 \sim 0.5$。</p>
<h2 id="182">18.2 重编译触发机制</h2>
<p>智能的重编译触发机制是平衡性能和资源消耗的关键。</p>
<h3 id="1821">18.2.1 触发条件设计</h3>
<p><strong>多维触发条件</strong>：</p>
<p>定义触发向量 $\vec{T} = (T_{\text{freq}}, T_{\text{perf}}, T_{\text{mem}}, T_{\text{shape}})$：</p>
<ul>
<li>$T_{\text{freq}}$：执行频率触发</li>
<li>$T_{\text{perf}}$：性能退化触发</li>
<li>$T_{\text{mem}}$：内存压力触发</li>
<li>$T_{\text{shape}}$：形状变化触发</li>
</ul>
<p>触发决策函数：
$$\text{trigger} = \bigvee_{i} (T_i &gt; \theta_i) \vee \left(\sum_{i} w_i \cdot T_i &gt; \theta_{\text{global}}\right)$$</p>
<h3 id="1822">18.2.2 阈值自适应调整</h3>
<p><strong>指数移动平均调整</strong>：
$$\theta_{\text{new}} = \begin{cases}
\theta_{\text{old}} \times (1 + \beta) &amp; \text{if } \text{benefit} &gt; \text{cost} \\
\theta_{\text{old}} \times (1 - \beta) &amp; \text{otherwise}
\end{cases}$$
其中 $\beta$ 是调整步长，通常取 $0.1$。</p>
<p><strong>基于强化学习的阈值优化</strong>：</p>
<p>将阈值调整建模为马尔可夫决策过程（MDP）：</p>
<ul>
<li>状态：$s = (\text{workload}, \text{resources}, \text{history})$</li>
<li>动作：$a = \Delta\theta$</li>
<li>奖励：$r = \text{speedup} - \lambda \cdot \text{compile_cost}$</li>
</ul>
<p>使用 Q-learning 更新策略：
$$Q(s,a) \leftarrow (1-\alpha)Q(s,a) + \alpha[r + \gamma \max_{a'} Q(s',a')]$$</p>
<h3 id="1823">18.2.3 热点检测算法</h3>
<p><strong>滑动窗口热点检测</strong>：</p>
<p>维护时间窗口 $W = [t-\tau, t]$ 内的执行统计：
$$H(op, s) = \frac{\sum_{i \in W} \mathbb{1}[\text{shape}_i = s] \cdot \text{time}_i}{\sum_{i \in W} \text{time}_i}$$
当 $H(op, s) &gt; \theta_{\text{hot}}$ 时，标记为热点。</p>
<p><strong>自适应采样</strong>：</p>
<p>使用 reservoir sampling 降低统计开销：
$$P_{\text{sample}} = \min\left(1, \frac{k}{\text{count}}\right)$$
其中 $k$ 是 reservoir 大小。</p>
<h2 id="183-shape">18.3 Shape 缓存策略</h2>
<p>有效的缓存策略可以显著减少重编译开销，提高动态 shape 系统的整体性能。</p>
<h3 id="1831">18.3.1 缓存结构设计</h3>
<p><strong>多级缓存架构</strong>：</p>
<div class="codehilite"><pre><span></span><code>L1<span class="w"> </span>Cache<span class="w"> </span>(Per-Op)
├──<span class="w"> </span>Exact<span class="w"> </span>Match<span class="w"> </span>Cache
│<span class="w">   </span>└──<span class="w"> </span>Hash<span class="w"> </span>Table:<span class="w"> </span>shape<span class="w"> </span>→<span class="w"> </span>compiled_code
├──<span class="w"> </span>Pattern<span class="w"> </span>Cache<span class="w">  </span>
│<span class="w">   </span>└──<span class="w"> </span>Trie:<span class="w"> </span>shape_pattern<span class="w"> </span>→<span class="w"> </span>template_code
└──<span class="w"> </span>Range<span class="w"> </span>Cache
<span class="w">    </span>└──<span class="w"> </span>Interval<span class="w"> </span>Tree:<span class="w"> </span>shape_range<span class="w"> </span>→<span class="w"> </span>parametric_code

L2<span class="w"> </span>Cache<span class="w"> </span>(Global)
├──<span class="w"> </span>Shared<span class="w"> </span>Code<span class="w"> </span>Cache
└──<span class="w"> </span>Cross-Op<span class="w"> </span>Optimization<span class="w"> </span>Cache
</code></pre></div>

<p><strong>缓存键设计</strong>：</p>
<p>对于形状 $s = (d_1, d_2, ..., d_n)$，缓存键计算：
$$\text{key}(s) = \text{hash}(s) \oplus \text{hash}(\text{dtype}) \oplus \text{hash}(\text{layout})$$
引入局部敏感哈希（LSH）支持相似形状查找：
$$\text{LSH}(s) = \left\lfloor \frac{s}{g} \right\rfloor$$
其中 $g$ 是粒度参数。</p>
<p><strong>版本管理</strong>：</p>
<p>每个缓存项包含版本信息：
$$\text{entry} = \{\text{code}, \text{version}, \text{deps}, \text{stats}\}$$
版本兼容性检查：
$$\text{compatible}(v_1, v_2) = \bigwedge_{d \in \text{deps}} (\text{version}(d) = \text{version}_{\text{cached}}(d))$$</p>
<h3 id="1832">18.3.2 替换算法</h3>
<p><strong>自适应 LRU-K</strong>：</p>
<p>结合访问频率和最近性：
$$\text{priority}(e) = \sum_{i=1}^{K} w_i \cdot \frac{1}{t - t_i(e)}$$
其中 $t_i(e)$ 是倒数第 $i$ 次访问时间。</p>
<p><strong>成本感知替换</strong>：</p>
<p>考虑编译成本的替换决策：
$$\text{evict_score}(e) = \frac{\text{age}(e) \cdot \text{size}(e)}{\text{freq}(e) \cdot \text{compile_cost}(e)}$$
选择 score 最高的项进行替换。</p>
<p><strong>预测性预取</strong>：</p>
<p>基于形状序列预测：
$$P(s_{t+1} | s_t, s_{t-1}, ...) = \frac{\text{count}(s_t \rightarrow s_{t+1})}{\text{count}(s_t)}$$
当 $P(s') &gt; \theta_{\text{prefetch}}$ 时，预编译形状 $s'$。</p>
<h3 id="1833">18.3.3 缓存有效性验证</h3>
<p><strong>增量验证</strong>：
$$\text{valid}(c) = \text{checksum}(c) = \text{expected} \wedge \forall d \in \text{deps}(c): \text{unchanged}(d)$$
<strong>依赖追踪图</strong>：</p>
<p>构建缓存项之间的依赖关系：</p>
<div class="codehilite"><pre><span></span><code>    Op1_Cache
    /        \
   v          v
Op2_Cache  Op3_Cache
   \          /
    v        v
    Op4_Cache
</code></pre></div>

<p>当上游缓存失效时，递归标记下游缓存。</p>
<p><strong>一致性协议</strong>：</p>
<p>在分布式环境中，使用两阶段提交保证缓存一致性：</p>
<ol>
<li>Prepare: $\forall n \in \text{nodes}: \text{lock}(n, \text{key})$</li>
<li>Commit: $\forall n \in \text{nodes}: \text{update}(n, \text{key}, \text{value})$</li>
</ol>
<h2 id="184">18.4 性能预测模型</h2>
<p>准确的性能预测是优化决策的基础。</p>
<h3 id="1841">18.4.1 成本模型构建</h3>
<p><strong>分层成本模型</strong>：
$$C_{\text{total}} = C_{\text{compute}} + C_{\text{memory}} + C_{\text{comm}} + C_{\text{overhead}}$$
计算成本：
$$C_{\text{compute}} = \sum_{op} \text{FLOPS}(op) \times \frac{1}{\text{efficiency}(op, \text{hw})}$$
内存成本：
$$C_{\text{memory}} = \sum_{t} \frac{\text{size}(t)}{\text{BW}_{\text{level}(t)}} + \text{miss_penalty} \times P_{\text{miss}}$$
通信成本（for 分布式）：
$$C_{\text{comm}} = \alpha + \beta \times \text{msg_size} + \gamma \times \text{hop_count}$$</p>
<h3 id="1842">18.4.2 机器学习辅助预测</h3>
<p><strong>特征工程</strong>：</p>
<p>提取关键特征向量 $\vec{f}$：</p>
<ul>
<li>形状特征：$(N, C, H, W, \text{aspect_ratio}, \text{volume})$</li>
<li>算子特征：$(\text{type}, \text{params}, \text{memory_pattern})$</li>
<li>硬件特征：$(\text{compute_units}, \text{memory_bw}, \text{cache_size})$</li>
</ul>
<p><strong>XGBoost 预测模型</strong>：
$$T_{\text{pred}} = \sum_{k=1}^{K} f_k(\vec{f})$$
其中 $f_k$ 是第 $k$ 棵决策树。</p>
<p>训练目标：
$$\mathcal{L} = \sum_{i} (T_{\text{pred}}^{(i)} - T_{\text{actual}}^{(i)})^2 + \sum_{k} \Omega(f_k)$$
<strong>在线学习更新</strong>：</p>
<p>使用指数加权移动平均更新模型参数：
$$\theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta \mathcal{L}(\theta_t, x_t, y_t)$$</p>
<h3 id="1843">18.4.3 自适应调整机制</h3>
<p><strong>贝叶斯优化</strong>：</p>
<p>使用高斯过程建模性能函数：
$$f \sim \mathcal{GP}(\mu, k)$$
采集函数（Expected Improvement）：
$$\text{EI}(x) = \mathbb{E}[\max(f(x) - f^*, 0)]$$
其中 $f^*$ 是当前最优值。</p>
<p><strong>多臂老虎机策略</strong>：</p>
<p>平衡探索与利用：
$$a_t = \arg\max_a \left[ \hat{\mu}_a + \sqrt{\frac{2\ln t}{n_a}} \right]$$
其中 $\hat{\mu}_a$ 是动作 $a$ 的平均收益，$n_a$ 是选择次数。</p>
<p><strong>反馈控制环</strong>：</p>
<div class="codehilite"><pre><span></span><code>目标性能 → PID控制器 → 参数调整
    ↑                      ↓
实际性能 ← 系统执行 ← 新参数
</code></pre></div>

<p>PID 控制器：
$$u(t) = K_p e(t) + K_i \int_0^t e(\tau)d\tau + K_d \frac{de(t)}{dt}$$</p>
<h2 id="_1">本章小结</h2>
<p>本章深入探讨了动态 shape 编译的运行时技术，涵盖了从特化决策到性能预测的完整体系：</p>
<ol>
<li>
<p><strong>运行时特化</strong>：通过热度阈值模型和分层特化架构，在编译开销和执行效率之间找到平衡点。关键公式：$B(s) = (T_{\text{interp}}(s) - T_{\text{spec}}(s)) \times P_{\text{reuse}}(s) - C_{\text{compile}}(s)$</p>
</li>
<li>
<p><strong>重编译触发</strong>：设计了多维触发条件和自适应阈值调整机制，使用强化学习优化触发策略。核心是平衡探索与利用的权衡。</p>
</li>
<li>
<p><strong>缓存策略</strong>：构建了多级缓存架构，结合 LRU-K 和成本感知的替换算法，通过局部敏感哈希支持相似形状查找。</p>
</li>
<li>
<p><strong>性能预测</strong>：建立了分层成本模型 $C_{\text{total}} = C_{\text{compute}} + C_{\text{memory}} + C_{\text{comm}} + C_{\text{overhead}}$，并使用机器学习方法提高预测准确性。</p>
</li>
</ol>
<p>这些技术的综合应用，使得动态 shape 系统能够在保持灵活性的同时，达到接近静态 shape 系统 85-95% 的性能水平。</p>
<h2 id="_2">练习题</h2>
<h3 id="_3">基础题</h3>
<p><strong>练习 18.1</strong>：给定一个形状序列 $S = [(32,3,224,224), (64,3,224,224), (32,3,224,224), (128,3,224,224), (32,3,224,224)]$，热度阈值 $\theta_{\text{hot}} = 2$，计算哪些形状应该被特化？</p>
<p><em>Hint</em>：统计每个形状的出现频率，与阈值比较。</p>
<details>
<summary>答案</summary>
<p>形状 $(32,3,224,224)$ 出现 3 次，超过阈值 2，应该被特化。
其他形状各出现 1 次，不应被特化。</p>
</details>
<p><strong>练习 18.2</strong>：一个算子的解释执行时间为 100ms，特化后执行时间为 20ms，编译开销为 500ms。如果形状重用概率为 0.8，计算特化收益 $B(s)$。</p>
<p><em>Hint</em>：直接代入收益预测公式。</p>
<details>
<summary>答案</summary>
<p>$$B(s) = (100 - 20) \times 0.8 - 500 = 80 \times 0.8 - 500 = 64 - 500 = -436\text{ms}$$
收益为负，不应该特化。若要使收益为正，需要形状重用概率 &gt; 6.25。</p>
</details>
<p><strong>练习 18.3</strong>：设计一个简单的 LRU-2 缓存替换算法，缓存容量为 3，处理访问序列：A, B, C, D, B, A, E, B, C。列出每步后的缓存状态。</p>
<p><em>Hint</em>：LRU-2 考虑倒数第二次访问时间。</p>
<details>
<summary>答案</summary>
<p>步骤追踪：</p>
<ol>
<li>A → [A]</li>
<li>B → [A, B]</li>
<li>C → [A, B, C]</li>
<li>D → [B, C, D] (淘汰 A，因为 A 只访问过一次)</li>
<li>B → [B, C, D] (B 更新)</li>
<li>A → [B, C, A] (淘汰 D)</li>
<li>E → [B, A, E] (淘汰 C，C 的倒数第二次访问最早)</li>
<li>B → [B, A, E] (B 更新)</li>
<li>C → [B, A, C] (淘汰 E)</li>
</ol>
<p>最终缓存：[B, A, C]</p>
</details>
<h3 id="_4">挑战题</h3>
<p><strong>练习 18.4</strong>：设计一个自适应阈值调整算法。已知最近 10 次重编译的收益/成本比为：[0.5, 0.8, 1.2, 1.5, 0.9, 1.1, 1.3, 0.7, 1.4, 1.6]。初始阈值 $\theta = 100$，调整步长 $\beta = 0.1$。计算最终阈值。</p>
<p><em>Hint</em>：根据收益/成本比决定调整方向。</p>
<details>
<summary>答案</summary>
<p>分析每次调整：</p>
<ul>
<li>比值 &gt; 1 的次数：6 次（上调）</li>
<li>比值 ≤ 1 的次数：4 次（下调）</li>
</ul>
<p>累积调整：
$$\theta_{\text{final}} = 100 \times (1.1)^6 \times (0.9)^4 = 100 \times 1.772 \times 0.656 = 116.2$$
考虑到实际中可能使用移动平均或其他平滑策略，最终阈值约为 116。</p>
</details>
<p><strong>练习 18.5</strong>：给定一个具有以下特性的深度学习模型：</p>
<ul>
<li>输入形状在 $[1, 32] \times 3 \times [224, 512] \times [224, 512]$ 范围内变化</li>
<li>batch size 以概率 0.7 为 1，0.2 为 8，0.1 为 32</li>
<li>图像尺寸以概率 0.8 为 224×224，0.2 为 512×512</li>
</ul>
<p>设计一个桶化策略，使得缓存命中率最大化，同时限制桶数量不超过 6。</p>
<p><em>Hint</em>：考虑概率分布和性能影响。</p>
<details>
<summary>答案</summary>
<p>优化策略：</p>
<ol>
<li>Batch 维度：[1], [8], [32] - 3 个桶</li>
<li>空间维度：[224×224], [512×512] - 2 个桶</li>
</ol>
<p>组合后共 3×2 = 6 个桶：</p>
<ul>
<li>(1, 224×224) - 概率 0.7×0.8 = 0.56</li>
<li>(1, 512×512) - 概率 0.7×0.2 = 0.14</li>
<li>(8, 224×224) - 概率 0.2×0.8 = 0.16</li>
<li>(8, 512×512) - 概率 0.2×0.2 = 0.04</li>
<li>(32, 224×224) - 概率 0.1×0.8 = 0.08</li>
<li>(32, 512×512) - 概率 0.1×0.2 = 0.02</li>
</ul>
<p>理论缓存命中率 = 100%（所有情况都被覆盖）</p>
</details>
<p><strong>练习 18.6</strong>：使用贝叶斯优化选择最优编译参数。已知性能函数的三个观测点：</p>
<ul>
<li>$x_1 = 0.2$, $y_1 = 0.6$</li>
<li>$x_2 = 0.5$, $y_2 = 0.8$  </li>
<li>$x_3 = 0.8$, $y_3 = 0.7$</li>
</ul>
<p>假设高斯过程的均值为 0，核函数为 RBF。计算 $x = 0.6$ 处的期望改进（EI）。</p>
<p><em>Hint</em>：先估计该点的均值和方差。</p>
<details>
<summary>答案</summary>
<p>使用 RBF 核函数插值：</p>
<ol>
<li>计算协方差矩阵 K</li>
<li>预测 $x = 0.6$ 的均值：约 0.82</li>
<li>预测方差：约 0.05</li>
<li>当前最优 $f^* = 0.8$</li>
</ol>
<p>期望改进：
$$\text{EI}(0.6) = (0.82 - 0.8) \times \Phi\left(\frac{0.02}{\sqrt{0.05}}\right) + \sqrt{0.05} \times \phi\left(\frac{0.02}{\sqrt{0.05}}\right)$$
$$\approx 0.02 \times 0.54 + 0.22 \times 0.39 \approx 0.097$$</p>
<p>建议在 $x = 0.6$ 附近继续探索。</p>
</details>
<p><strong>练习 18.7</strong>：分析一个自动驾驶场景中的动态 shape 问题。检测到的目标数量在 [0, 50] 之间变化，每帧处理时间限制为 33ms。设计一个编译策略，确保 99% 的帧满足实时性要求。已知：</p>
<ul>
<li>目标数 ≤ 10 的概率：80%</li>
<li>目标数 11-30 的概率：15%</li>
<li>目标数 31-50 的概率：5%</li>
</ul>
<p><em>Hint</em>：考虑分层编译和降级策略。</p>
<details>
<summary>答案</summary>
<p>分层策略设计：</p>
<ol>
<li>
<p><strong>L0 层（快速路径）</strong>：
   - 针对 0-10 个目标深度优化
   - 执行时间：&lt; 20ms
   - 覆盖 80% 情况</p>
</li>
<li>
<p><strong>L1 层（标准路径）</strong>：
   - 针对 11-30 个目标优化
   - 执行时间：&lt; 30ms
   - 覆盖 15% 情况</p>
</li>
<li>
<p><strong>L2 层（降级路径）</strong>：
   - 31-50 个目标
   - 使用低精度/跳帧策略
   - 确保 &lt; 33ms</p>
</li>
</ol>
<p>时间预算分配：</p>
<ul>
<li>80% × 20ms + 15% × 30ms + 5% × 33ms = 22.15ms 平均延迟</li>
</ul>
<p>99 分位延迟 ≤ 33ms，满足要求。</p>
<p>降级策略：</p>
<ul>
<li>当目标数 &gt; 40 时，降低检测精度</li>
<li>当连续 2 帧超时，跳过非关键目标</li>
</ul>
</details>
<h2 id="_5">常见陷阱与错误</h2>
<ol>
<li>
<p><strong>过度特化陷阱</strong>：为每个轻微的形状变化都生成特化代码，导致编译开销爆炸和缓存污染。应该设置合理的相似度阈值。</p>
</li>
<li>
<p><strong>缓存一致性问题</strong>：在分布式环境中，不同节点的缓存可能不一致。必须实现严格的版本控制和同步机制。</p>
</li>
<li>
<p><strong>内存泄漏</strong>：动态编译的代码如果没有正确释放，会导致内存持续增长。需要实现代码生命周期管理。</p>
</li>
<li>
<p><strong>性能抖动</strong>：频繁的重编译会导致性能不稳定。应该实现编译决策的滞后机制。</p>
</li>
<li>
<p><strong>预测模型过拟合</strong>：在线学习的预测模型可能过度适应特定工作负载。需要定期重置或使用正则化。</p>
</li>
<li>
<p><strong>死锁风险</strong>：多线程环境下的缓存锁可能导致死锁。建议使用无锁数据结构或细粒度锁。</p>
</li>
<li>
<p><strong>形状爆炸</strong>：某些模型的形状空间极大（如 NLP 的变长输入）。必须实现有效的桶化和泛化策略。</p>
</li>
</ol>
<h2 id="_6">最佳实践检查清单</h2>
<h3 id="_7">设计阶段</h3>
<ul>
<li>[ ] 定义清晰的形状分类标准和相似度度量</li>
<li>[ ] 设计多级缓存架构，平衡命中率和内存占用</li>
<li>[ ] 建立性能模型，包括编译成本和执行收益</li>
<li>[ ] 规划降级策略，处理极端情况</li>
</ul>
<h3 id="_8">实现阶段</h3>
<ul>
<li>[ ] 实现高效的形状哈希和查找机制</li>
<li>[ ] 使用无锁或细粒度锁减少并发开销</li>
<li>[ ] 实现增量编译，复用已有编译结果</li>
<li>[ ] 添加性能计数器，跟踪关键指标</li>
</ul>
<h3 id="_9">优化阶段</h3>
<ul>
<li>[ ] 分析形状分布，优化桶化策略</li>
<li>[ ] 调优重编译阈值，平衡性能和开销</li>
<li>[ ] 实现预测性编译，提前准备热点形状</li>
<li>[ ] 优化内存布局，减少缓存失效</li>
</ul>
<h3 id="_10">监控阶段</h3>
<ul>
<li>[ ] 监控缓存命中率和编译频率</li>
<li>[ ] 跟踪 P50/P90/P99 延迟指标</li>
<li>[ ] 检测内存泄漏和资源耗尽</li>
<li>[ ] 分析性能瓶颈，持续优化</li>
</ul>
<h3 id="_11">测试阶段</h3>
<ul>
<li>[ ] 测试各种形状组合和变化模式</li>
<li>[ ] 验证缓存一致性和正确性</li>
<li>[ ] 压力测试，确保系统稳定性</li>
<li>[ ] 基准测试，对比静态编译性能</li>
</ul>
            </article>
            
            <nav class="page-nav"><a href="chapter17.html" class="nav-link prev">← 第 17 章：动态 Shape 编译（一）</a><a href="chapter19.html" class="nav-link next">第 19 章：稀疏与变长数据支持 →</a></nav>
        </main>
    </div>
</body>
</html>